{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIS 563 - Intro to Data Science <br>\n",
    "HW 2<br>\n",
    "Harper He (xhe128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/harperhe/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\")\n",
    "from nltk import FreqDist, WordNetLemmatizer\n",
    "from nltk.tokenize.regexp import wordpunct_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocess: <br>\n",
    "use WordNet as a dictionary to lemmatize words<br>\n",
    "use punctuation and number to split string<br>\n",
    "lowercase each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatizer = lambda x: wnl.lemmatize(str.strip(x, string.punctuation + \"0123456789\").lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two empty lists for review sentences and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "reviews_token = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset line by line for values in “review/text” field<br>\n",
    "Since the text file is not in utf-8 form, we need to add \"encoding\" parameter<br>\n",
    "We only keep the second half of the lines that start with \"review/text: \" and store them in the review list<br>\n",
    "We also tokenize each sentence and lowercase, lemmatize each word then store them in the review token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finefoods.txt\",encoding = \"ISO-8859-1\") as finefoods:\n",
    "    for line in finefoods:       \n",
    "        if(line.startswith(\"review/text: \")):\n",
    "            one_line = line.split(\"review/text: \")[1]\n",
    "            reviews.append(one_line)\n",
    "            reviews_token.extend(list(map(lemmatizer, wordpunct_tokenize(one_line))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify all the unique words that appear in the \"\\review/text\" field of the reviews. <br>\n",
    "Denote the set of such words as L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = set([word for word in reviews_token if word !=\"\" and word.isalpha() and word!= \"br\"and word!= \"\\n\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the stopwords list from http://www.ranks.nl/stopwords <br>\n",
    "The following codes kept reporting 'certificate verify failed' so I copied the stopwords manually from the script of the website<br>\n",
    "link = \"https://www.ranks.nl/stopwords\"<br>\n",
    "stopwordlink = urlopen(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_raw = \"a<br />able<br />about<br />above<br />abst<br />accordance<br />according<br />accordingly<br />across<br />act<br />actually<br />added<br />adj<br />affected<br />affecting<br />affects<br />after<br />afterwards<br />again<br />against<br />ah<br />all<br />almost<br />alone<br />along<br />already<br />also<br />although<br />always<br />am<br />among<br />amongst<br />an<br />and<br />announce<br />another<br />any<br />anybody<br />anyhow<br />anymore<br />anyone<br />anything<br />anyway<br />anyways<br />anywhere<br />apparently<br />approximately<br />are<br />aren<br />arent<br />arise<br />around<br />as<br />aside<br />ask<br />asking<br />at<br />auth<br />available<br />away<br />awfully<br />b<br />back<br />be<br />became<br />because<br />become<br />becomes<br />becoming<br />been<br />before<br />beforehand<br />begin<br />beginning<br />beginnings<br />begins<br />behind<br />being<br />believe<br />below<br />beside<br />besides<br />between<br />beyond<br />biol<br />both<br />brief<br />briefly<br />but<br />by<br />c<br />ca<br />came<br />can<br />cannot<br />can't<br />cause<br />causes<br />certain<br />certainly<br />co<br />com<br />come<br />comes<br />contain<br />containing<br />contains<br />could<br />couldnt<br />d<br />date<br />did<br />didn't<br />different<br />do<br />does<br />doesn't<br />doing<br />done<br />don't<br />down<br />downwards<br />due<br />during<br />e<br />each<br />ed<br />edu<br />effect<br />eg<br />eight<br />eighty<br />either<br />else<br />elsewhere<br />end<br />ending<br />enough<br />especially<br />et<br />et-al<br />etc<br />even<br />ever<br />every<br />everybody<br />everyone<br />everything<br />everywhere<br />ex<br />except<br />f<br />far<br />few<br />ff<br />fifth<br />first<br />five<br />fix<br />followed<br />following<br />follows<br />for<br />former<br />formerly<br />forth<br />found<br />four<br />from<br />further<br />furthermore<br />g<br />gave<br />get<br />gets<br />getting<br />give<br />given<br />gives<br />giving<br />go<br />goes<br />gone<br />got<br />gotten<br />h<br />had<br />happens<br />hardly<br />has<br />hasn't<br />have<br />haven't<br />having<br />he<br />hed<br />hence<br />her<br />here<br />hereafter<br />hereby<br />herein<br />heres<br />hereupon<br />hers<br />herself<br />hes<br />hi<br />hid<br />him<br />himself<br />his<br />hither<br />home<br />how<br />howbeit<br />however<br />hundred<br />i<br />id<br />ie<br />if<br />i'll<br />im<br />immediate<br />immediately<br />importance<br />important<br />in<br />inc<br />indeed<br />index<br />information<br />instead<br />into<br />invention<br />inward<br />is<br />isn't<br />it<br />itd<br />it'll<br />its<br />itself<br />i've<br />j<br />just<br />k<br />keep</td>keeps<br />kept<br />kg<br />km<br />know<br />known<br />knows<br />l<br />largely<br />last<br />lately<br />later<br />latter<br />latterly<br />least<br />less<br />lest<br />let<br />lets<br />like<br />liked<br />likely<br />line<br />little<br />'ll<br />look<br />looking<br />looks<br />ltd<br />m<br />made<br />mainly<br />make<br />makes<br />many<br />may<br />maybe<br />me<br />mean<br />means<br />meantime<br />meanwhile<br />merely<br />mg<br />might<br />million<br />miss<br />ml<br />more<br />moreover<br />most<br />mostly<br />mr<br />mrs<br />much<br />mug<br />must<br />my<br />myself<br />n<br />na<br />name<br />namely<br />nay<br />nd<br />near<br />nearly<br />necessarily<br />necessary<br />need<br />needs<br />neither<br />never<br />nevertheless<br />new<br />next<br />nine<br />ninety<br />no<br />nobody<br />non<br />none<br />nonetheless<br />noone<br />nor<br />normally<br />nos<br />not<br />noted<br />nothing<br />now<br />nowhere<br />o<br />obtain<br />obtained<br />obviously<br />of<br />off<br />often<br />oh<br />ok<br />okay<br />old<br />omitted<br />on<br />once<br />one<br />ones<br />only<br />onto<br />or<br />ord<br />other<br />others<br />otherwise<br />ought<br />our<br />ours<br />ourselves<br />out<br />outside<br />over<br />overall<br />owing<br />own<br />p<br />page<br />pages<br />part<br />particular<br />particularly<br />past<br />per<br />perhaps<br />placed<br />please<br />plus<br />poorly<br />possible<br />possibly<br />potentially<br />pp<br />predominantly<br />present<br />previously<br />primarily<br />probably<br />promptly<br />proud<br />provides<br />put<br />q<br />que<br />quickly<br />quite<br />qv<br />r<br />ran<br />rather<br />rd<br />re<br />readily<br />really<br />recent<br />recently<br />ref<br />refs<br />regarding<br />regardless<br />regards<br />related<br />relatively<br />research<br />respectively<br />resulted<br />resulting<br />results<br />right<br />run<br />s<br />said<br />same<br />saw<br />say<br />saying<br />says<br />sec<br />section<br />see<br />seeing<br />seem<br />seemed<br />seeming<br />seems<br />seen<br />self<br />selves<br />sent<br />seven<br />several<br />shall<br />she<br />shed<br />she'll<br />shes<br />should<br />shouldn't<br />show<br />showed<br />shown<br />showns<br />shows<br />significant<br />significantly<br />similar<br />similarly<br />since<br />six<br />slightly<br />so<br />some<br />somebody<br />somehow<br />someone<br />somethan<br />something<br />sometime<br />sometimes<br />somewhat<br />somewhere<br />soon<br />sorry<br />specifically<br />specified<br />specify<br />specifying<br />still<br />stop<br />strongly<br />sub<br />substantially<br />successfully<br />such<br />sufficiently<br />suggest<br />sup<br />sure</td>t<br />take<br />taken<br />taking<br />tell<br />tends<br />th<br />than<br />thank<br />thanks<br />thanx<br />that<br />that'll<br />thats<br />that've<br />the<br />their<br />theirs<br />them<br />themselves<br />then<br />thence<br />there<br />thereafter<br />thereby<br />thered<br />therefore<br />therein<br />there'll<br />thereof<br />therere<br />theres<br />thereto<br />thereupon<br />there've<br />these<br />they<br />theyd<br />they'll<br />theyre<br />they've<br />think<br />this<br />those<br />thou<br />though<br />thoughh<br />thousand<br />throug<br />through<br />throughout<br />thru<br />thus<br />til<br />tip<br />to<br />together<br />too<br />took<br />toward<br />towards<br />tried<br />tries<br />truly<br />try<br />trying<br />ts<br />twice<br />two<br />u<br />un<br />under<br />unfortunately<br />unless<br />unlike<br />unlikely<br />until<br />unto<br />up<br />upon<br />ups<br />us<br />use<br />used<br />useful<br />usefully<br />usefulness<br />uses<br />using<br />usually<br />v<br />value<br />various<br />'ve<br />very<br />via<br />viz<br />vol<br />vols<br />vs<br />w<br />want<br />wants<br />was<br />wasnt<br />way<br />we<br />wed<br />welcome<br />we'll<br />went<br />were<br />werent<br />we've<br />what<br />whatever<br />what'll<br />whats<br />when<br />whence<br />whenever<br />where<br />whereafter<br />whereas<br />whereby<br />wherein<br />wheres<br />whereupon<br />wherever<br />whether<br />which<br />while<br />whim<br />whither<br />who<br />whod<br />whoever<br />whole<br />who'll<br />whom<br />whomever<br />whos<br />whose<br />why<br />widely<br />willing<br />wish<br />with<br />within<br />without<br />wont<br />words<br />world<br />would<br />wouldnt<br />www<br />x<br />y<br />yes<br />yet<br />you<br />youd<br />you'll<br />your<br />youre<br />yours<br />yourself<br />yourselves<br />you've<br />z<br />zero\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I split stopwords and made them a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords_raw.replace('</td>','<br />').split(\"<br />\")\n",
    "stopwords = list(map(lemmatizer, stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote the cleaned set as W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [word for word in L if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of times each word in W appears among all reviews \"\"\\review/text\" field and identify the top 500 words.<br>\n",
    "Creat 2 empty lists for top 500 words and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500 = []\n",
    "top_500_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of times each word appears among all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDistribution = FreqDist(reviews_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count and identify the top 500 words that are in W appear among all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 0 \n",
    "for word, count in freqDistribution.most_common():\n",
    "    if(word in W):\n",
    "        top_500.append(word)\n",
    "        top_500_count.append(count)\n",
    "        top_words = top_words + 1\n",
    "        if(top_words == 500):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize all reviews in \"review/text\" field using these 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize all reviews (“review/text” field) using these 500 words\n",
    "vectorizer = CountVectorizer(vocabulary = top_500)\n",
    "V = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the vectorized reviews into 10 clusters using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state = 0).fit(normalize(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each centroid, select the top 5 words that represent the centroid (i.e., the words with the highest feature values)<br>\n",
    "create two empty lists for top 5 words that represent the centroid as well as their feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = []\n",
    "top_5_feature_values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster, sort the centroid to get the first/top 5 words as well as their feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_centroid = kmeans.cluster_centers_.argsort()[:,::-1]\n",
    "# argsort returns the indices that would sort an array.\n",
    "\n",
    "for i in range(len(sorted_centroid)):\n",
    "    \n",
    "    word_centroid = []\n",
    "    feature_values = []\n",
    "    \n",
    "    for word in list(sorted_centroid[i, :5]):\n",
    "        \n",
    "        word_centroid.append(top_500[word])\n",
    "        feature_values.append(kmeans.cluster_centers_[i,word])\n",
    "        \n",
    "    top_5.append(word_centroid)\n",
    "    top_5_feature_values.append(feature_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print final Results for top 500 words and top 5 words representing each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 500 Words and counts of these words:\n",
      "\n",
      "taste  -  382476\n",
      "good  -  355021\n",
      "product  -  352214\n",
      "flavor  -  327715\n",
      "coffee  -  308148\n",
      "great  -  293048\n",
      "love  -  287758\n",
      "food  -  270005\n",
      "tea  -  266757\n",
      "will  -  223419\n",
      "dog  -  191800\n",
      "amazon  -  186587\n",
      "time  -  178525\n",
      "don  -  161259\n",
      "cup  -  157213\n",
      "bag  -  153571\n",
      "price  -  142381\n",
      "buy  -  135678\n",
      "best  -  134374\n",
      "find  -  129832\n",
      "well  -  129225\n",
      "better  -  124464\n",
      "treat  -  120586\n",
      "day  -  120266\n",
      "eat  -  119359\n",
      "store  -  118403\n",
      "box  -  116586\n",
      "cat  -  115546\n",
      "chocolate  -  114284\n",
      "drink  -  107687\n",
      "water  -  106517\n",
      "year  -  106490\n",
      "sugar  -  106095\n",
      "brand  -  103251\n",
      "sweet  -  94834\n",
      "free  -  89412\n",
      "bought  -  88205\n",
      "thing  -  87472\n",
      "bit  -  83163\n",
      "order  -  80952\n",
      "ingredient  -  78995\n",
      "favorite  -  78928\n",
      "mix  -  78466\n",
      "chip  -  77324\n",
      "lot  -  77120\n",
      "snack  -  76190\n",
      "recommend  -  74858\n",
      "work  -  74654\n",
      "bar  -  74290\n",
      "pack  -  73123\n",
      "delicious  -  72706\n",
      "nice  -  72700\n",
      "add  -  69696\n",
      "hot  -  66834\n",
      "stuff  -  65478\n",
      "review  -  64771\n",
      "organic  -  63880\n",
      "quality  -  63467\n",
      "healthy  -  63146\n",
      "didn  -  62019\n",
      "milk  -  61540\n",
      "package  -  60063\n",
      "small  -  59696\n",
      "doesn  -  59048\n",
      "month  -  58381\n",
      "oil  -  58302\n",
      "perfect  -  57693\n",
      "calorie  -  57582\n",
      "ordered  -  57265\n",
      "bad  -  56630\n",
      "strong  -  56552\n",
      "eating  -  56438\n",
      "salt  -  55470\n",
      "easy  -  55222\n",
      "green  -  55190\n",
      "smell  -  54617\n",
      "fresh  -  53805\n",
      "problem  -  53670\n",
      "hard  -  53611\n",
      "long  -  53560\n",
      "high  -  53314\n",
      "size  -  52837\n",
      "natural  -  52522\n",
      "enjoy  -  52191\n",
      "chicken  -  51472\n",
      "definitely  -  51318\n",
      "buying  -  51196\n",
      "thought  -  51131\n",
      "regular  -  50983\n",
      "sauce  -  50870\n",
      "cooky  -  50745\n",
      "bottle  -  49538\n",
      "pretty  -  48489\n",
      "diet  -  48183\n",
      "happy  -  47734\n",
      "local  -  47673\n",
      "star  -  46819\n",
      "feel  -  46793\n",
      "people  -  46641\n",
      "big  -  46361\n",
      "going  -  45855\n",
      "tasty  -  45808\n",
      "fat  -  45564\n",
      "shipping  -  44656\n",
      "butter  -  43643\n",
      "kind  -  43474\n",
      "real  -  43441\n",
      "purchase  -  42898\n",
      "wonderful  -  42575\n",
      "amount  -  42467\n",
      "highly  -  42270\n",
      "low  -  42167\n",
      "texture  -  42154\n",
      "fruit  -  42149\n",
      "oz  -  42054\n",
      "dry  -  41945\n",
      "item  -  41889\n",
      "tasted  -  41149\n",
      "week  -  40998\n",
      "gluten  -  40975\n",
      "purchased  -  40746\n",
      "blend  -  40638\n",
      "protein  -  40599\n",
      "excellent  -  40531\n",
      "tasting  -  40177\n",
      "bean  -  40142\n",
      "worth  -  40072\n",
      "half  -  39957\n",
      "variety  -  39804\n",
      "coconut  -  39691\n",
      "expensive  -  39498\n",
      "candy  -  39357\n",
      "morning  -  39175\n",
      "rice  -  38816\n",
      "cereal  -  38777\n",
      "help  -  38680\n",
      "case  -  38605\n",
      "grocery  -  37889\n",
      "full  -  37870\n",
      "company  -  37806\n",
      "dark  -  37614\n",
      "peanut  -  37498\n",
      "vanilla  -  36618\n",
      "kid  -  36226\n",
      "family  -  35499\n",
      "friend  -  35474\n",
      "meal  -  35136\n",
      "loved  -  34635\n",
      "piece  -  34254\n",
      "received  -  34139\n",
      "flavored  -  34075\n",
      "three  -  33788\n",
      "won  -  33684\n",
      "health  -  33482\n",
      "ounce  -  33270\n",
      "cost  -  32941\n",
      "top  -  32264\n",
      "money  -  32230\n",
      "light  -  32192\n",
      "serving  -  32150\n",
      "black  -  31996\n",
      "minute  -  31835\n",
      "isn  -  31364\n",
      "making  -  31123\n",
      "large  -  31076\n",
      "save  -  30877\n",
      "drinking  -  30496\n",
      "bitter  -  30331\n",
      "fact  -  30019\n",
      "arrived  -  29964\n",
      "baby  -  29872\n",
      "cheese  -  29843\n",
      "corn  -  29334\n",
      "extra  -  29324\n",
      "husband  -  29239\n",
      "nut  -  29233\n",
      "disappointed  -  29011\n",
      "type  -  28944\n",
      "packaging  -  28942\n",
      "deal  -  28640\n",
      "http  -  28560\n",
      "fine  -  28497\n",
      "juice  -  28454\n",
      "honey  -  28290\n",
      "roast  -  28269\n",
      "gp  -  28031\n",
      "almond  -  28007\n",
      "syrup  -  27805\n",
      "meat  -  27769\n",
      "href  -  27767\n",
      "cream  -  27581\n",
      "second  -  27572\n",
      "recommended  -  27564\n",
      "decided  -  27523\n",
      "recipe  -  27392\n",
      "soup  -  27388\n",
      "started  -  27375\n",
      "spice  -  27350\n",
      "absolutely  -  27277\n",
      "house  -  27247\n",
      "potato  -  27159\n",
      "chew  -  27093\n",
      "prefer  -  27016\n",
      "energy  -  26904\n",
      "popcorn  -  26896\n",
      "smooth  -  26757\n",
      "bread  -  26643\n",
      "couple  -  26528\n",
      "grain  -  26482\n",
      "hour  -  26394\n",
      "breakfast  -  26358\n",
      "powder  -  26205\n",
      "pod  -  26082\n",
      "open  -  26072\n",
      "hand  -  25769\n",
      "cheaper  -  25753\n",
      "wanted  -  25681\n",
      "white  -  25662\n",
      "ago  -  25512\n",
      "read  -  25405\n",
      "packet  -  25116\n",
      "wasn  -  24835\n",
      "side  -  24467\n",
      "rich  -  24398\n",
      "longer  -  24183\n",
      "stick  -  24136\n",
      "container  -  24031\n",
      "gift  -  23918\n",
      "bold  -  23846\n",
      "pet  -  23706\n",
      "fan  -  23670\n",
      "red  -  23430\n",
      "ordering  -  23421\n",
      "cookie  -  23411\n",
      "amazing  -  23352\n",
      "life  -  23284\n",
      "mouth  -  23255\n",
      "soft  -  23182\n",
      "weight  -  23155\n",
      "fast  -  23131\n",
      "jar  -  23113\n",
      "issue  -  22809\n",
      "pound  -  22723\n",
      "cracker  -  22622\n",
      "reason  -  22612\n",
      "keurig  -  22581\n",
      "spicy  -  22409\n",
      "brew  -  22317\n",
      "pasta  -  22181\n",
      "son  -  21801\n",
      "couldn  -  21722\n",
      "quick  -  21695\n",
      "leaf  -  21693\n",
      "wheat  -  21614\n",
      "plastic  -  21446\n",
      "glad  -  21354\n",
      "difference  -  21340\n",
      "start  -  21161\n",
      "choice  -  21149\n",
      "color  -  21060\n",
      "version  -  20935\n",
      "beef  -  20929\n",
      "dried  -  20899\n",
      "market  -  20872\n",
      "ground  -  20840\n",
      "starbucks  -  20834\n",
      "place  -  20765\n",
      "ginger  -  20759\n",
      "lb  -  20651\n",
      "machine  -  20581\n",
      "hair  -  20568\n",
      "jerky  -  20382\n",
      "cinnamon  -  20365\n",
      "cold  -  20333\n",
      "crunchy  -  20128\n",
      "bowl  -  20069\n",
      "bite  -  19950\n",
      "seed  -  19837\n",
      "super  -  19820\n",
      "cut  -  19790\n",
      "feed  -  19737\n",
      "formula  -  19688\n",
      "apple  -  19648\n",
      "wouldn  -  19645\n",
      "mixed  -  19635\n",
      "vitamin  -  19630\n",
      "ice  -  19594\n",
      "teeth  -  19512\n",
      "opened  -  19417\n",
      "alternative  -  19344\n",
      "vet  -  19305\n",
      "cake  -  19273\n",
      "night  -  19241\n",
      "pepper  -  19232\n",
      "daughter  -  19116\n",
      "canned  -  19104\n",
      "care  -  19083\n",
      "fiber  -  18867\n",
      "lemon  -  18839\n",
      "online  -  18721\n",
      "easily  -  18531\n",
      "salty  -  18528\n",
      "brown  -  18500\n",
      "noodle  -  18474\n",
      "cook  -  18463\n",
      "haven  -  18418\n",
      "cooking  -  18237\n",
      "single  -  18214\n",
      "soy  -  18170\n",
      "clean  -  18072\n",
      "left  -  18021\n",
      "hope  -  18003\n",
      "soda  -  17994\n",
      "change  -  17985\n",
      "artificial  -  17983\n",
      "yummy  -  17913\n",
      "smaller  -  17893\n",
      "enjoyed  -  17835\n",
      "blue  -  17817\n",
      "guess  -  17683\n",
      "list  -  17507\n",
      "reviewer  -  17296\n",
      "french  -  17270\n",
      "flour  -  17217\n",
      "course  -  17208\n",
      "plain  -  17146\n",
      "awesome  -  17124\n",
      "body  -  17037\n",
      "gum  -  16981\n",
      "sweetener  -  16981\n",
      "content  -  16955\n",
      "oatmeal  -  16872\n",
      "original  -  16847\n",
      "cocoa  -  16818\n",
      "sodium  -  16814\n",
      "experience  -  16786\n",
      "exactly  -  16756\n",
      "dish  -  16724\n",
      "wife  -  16675\n",
      "aroma  -  16666\n",
      "wrong  -  16618\n",
      "aftertaste  -  16606\n",
      "caffeine  -  16594\n",
      "daily  -  16491\n",
      "special  -  16458\n",
      "pay  -  16440\n",
      "decaf  -  16432\n",
      "surprised  -  16388\n",
      "stomach  -  16335\n",
      "close  -  16317\n",
      "orange  -  16283\n",
      "bone  -  16202\n",
      "carry  -  16189\n",
      "pleased  -  16126\n",
      "simply  -  16077\n",
      "label  -  16029\n",
      "ate  -  16023\n",
      "pop  -  15976\n",
      "inside  -  15960\n",
      "heat  -  15917\n",
      "feeding  -  15907\n",
      "picky  -  15798\n",
      "idea  -  15748\n",
      "benefit  -  15747\n",
      "subscribe  -  15728\n",
      "compared  -  15707\n",
      "mild  -  15651\n",
      "point  -  15443\n",
      "finally  -  15439\n",
      "delivery  -  15395\n",
      "note  -  15306\n",
      "olive  -  15302\n",
      "service  -  15276\n",
      "huge  -  15269\n",
      "flavorful  -  15267\n",
      "medium  -  15194\n",
      "fish  -  15186\n",
      "maker  -  15151\n",
      "noticed  -  15141\n",
      "gram  -  15072\n",
      "looked  -  15064\n",
      "based  -  15011\n",
      "sell  -  14965\n",
      "option  -  14957\n",
      "salad  -  14914\n",
      "toy  -  14885\n",
      "instant  -  14816\n",
      "needed  -  14799\n",
      "live  -  14714\n",
      "customer  -  14677\n",
      "expected  -  14647\n",
      "pot  -  14582\n",
      "espresso  -  14551\n",
      "skin  -  14538\n",
      "pill  -  14502\n",
      "cherry  -  14415\n",
      "pouch  -  14399\n",
      "leave  -  14339\n",
      "child  -  14327\n",
      "bottom  -  14323\n",
      "packaged  -  14307\n",
      "larger  -  14261\n",
      "adding  -  14218\n",
      "allergy  -  14193\n",
      "stay  -  14154\n",
      "raw  -  14143\n",
      "egg  -  14070\n",
      "completely  -  14053\n",
      "lunch  -  14049\n",
      "bulk  -  13996\n",
      "convenient  -  13993\n",
      "vegetable  -  13966\n",
      "puppy  -  13942\n",
      "granola  -  13939\n",
      "fantastic  -  13930\n",
      "mind  -  13893\n",
      "mint  -  13876\n",
      "seasoning  -  13845\n",
      "weak  -  13843\n",
      "cheap  -  13779\n",
      "continue  -  13738\n",
      "glass  -  13722\n",
      "iced  -  13692\n",
      "stock  -  13687\n",
      "filling  -  13563\n",
      "opinion  -  13476\n",
      "rest  -  13417\n",
      "eaten  -  13411\n",
      "mine  -  13376\n",
      "chewy  -  13372\n",
      "turn  -  13300\n",
      "extremely  -  13258\n",
      "expect  -  13252\n",
      "healthier  -  13217\n",
      "batch  -  13200\n",
      "sale  -  13100\n",
      "hint  -  13079\n",
      "wellness  -  13064\n",
      "waste  -  13017\n",
      "shipped  -  13011\n",
      "true  -  12989\n",
      "count  -  12963\n",
      "excited  -  12873\n",
      "chai  -  12851\n",
      "strawberry  -  12849\n",
      "hit  -  12836\n",
      "break  -  12836\n",
      "normal  -  12775\n",
      "dinner  -  12754\n",
      "liquid  -  12731\n",
      "told  -  12714\n",
      "crunch  -  12640\n",
      "sold  -  12618\n",
      "wait  -  12443\n",
      "called  -  12434\n",
      "thick  -  12380\n",
      "call  -  12338\n",
      "sweetness  -  12293\n",
      "entire  -  12283\n",
      "fit  -  12209\n",
      "eats  -  12134\n",
      "today  -  12095\n",
      "baking  -  12058\n",
      "level  -  12022\n",
      "veggie  -  11929\n",
      "shop  -  11903\n",
      "set  -  11899\n",
      "seller  -  11854\n",
      "oat  -  11845\n",
      "source  -  11824\n",
      "purchasing  -  11811\n",
      "mountain  -  11811\n",
      "tomato  -  11648\n",
      "creamy  -  11625\n",
      "reading  -  11602\n",
      "easier  -  11573\n",
      "beat  -  11538\n",
      "stash  -  11533\n",
      "shipment  -  11526\n",
      "berry  -  11516\n",
      "chemical  -  11490\n",
      "difficult  -  11439\n",
      "pick  -  11414\n",
      "lower  -  11394\n",
      "christmas  -  11320\n",
      "supermarket  -  11313\n",
      "addition  -  11309\n",
      "blueberry  -  11304\n",
      "acid  -  11301\n",
      "pure  -  11292\n",
      "hold  -  11267\n",
      "drinker  -  11247\n",
      "offer  -  11226\n",
      "tiny  -  11205\n",
      "delivered  -  11195\n",
      "decent  -  11156\n",
      "matter  -  11153\n",
      "shake  -  11102\n",
      "substitute  -  11066\n",
      "sour  -  11052\n",
      "\n",
      "Top 5 words representing each cluster and  their feature values:\n",
      "\n",
      "Cluster 1 :\n",
      "\n",
      "['coffee', 'cup', 'good', 'flavor', 'taste']\n",
      "[0.514345928551566, 0.11160445313909816, 0.07063285904504546, 0.0636473692538847, 0.06285583898663231]\n",
      "\n",
      "Cluster 2 :\n",
      "\n",
      "['good', 'product', 'great', 'price', 'will']\n",
      "[0.39341335883441864, 0.04803487598956602, 0.04599772708414753, 0.04567294051353497, 0.04082106981827351]\n",
      "\n",
      "Cluster 3 :\n",
      "\n",
      "['tea', 'taste', 'good', 'flavor', 'green']\n",
      "[0.5712121280630085, 0.0626332449909312, 0.06152864026043901, 0.06092608450317598, 0.060894658828464544]\n",
      "\n",
      "Cluster 4 :\n",
      "\n",
      "['taste', 'good', 'great', 'will', 'don']\n",
      "[0.3444721111510588, 0.08695895856692107, 0.07754070839448124, 0.04120666936116848, 0.040991257077194546]\n",
      "\n",
      "Cluster 5 :\n",
      "\n",
      "['dog', 'food', 'treat', 'will', 'good']\n",
      "[0.4500156089725906, 0.10712915670549741, 0.07735364679019256, 0.05957003806272058, 0.05069241010805177]\n",
      "\n",
      "Cluster 6 :\n",
      "\n",
      "['flavor', 'great', 'good', 'taste', 'love']\n",
      "[0.33594947910940853, 0.07872822877620242, 0.06944767918302222, 0.05439185167581124, 0.04842968601190335]\n",
      "\n",
      "Cluster 7 :\n",
      "\n",
      "['product', 'great', 'amazon', 'will', 'price']\n",
      "[0.38382401232763896, 0.09886002452492443, 0.08104307122726093, 0.052659852935130175, 0.04113223078148526]\n",
      "\n",
      "Cluster 8 :\n",
      "\n",
      "['love', 'great', 'will', 'product', 'good']\n",
      "[0.368047485627743, 0.08886956166029644, 0.04579059768845526, 0.04577415607552348, 0.04493898513475536]\n",
      "\n",
      "Cluster 9 :\n",
      "\n",
      "['food', 'cat', 'eat', 'dog', 'will']\n",
      "[0.46668124982540893, 0.1410636748307551, 0.07401789395599728, 0.053466190122789296, 0.05292539162810279]\n",
      "\n",
      "Cluster 10 :\n",
      "\n",
      "['great', 'will', 'amazon', 'best', 'time']\n",
      "[0.07209123418078968, 0.048920549986320304, 0.042303463739013614, 0.03898765029172814, 0.03572004157347454]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 500 Words and counts of these words:\\n\")\n",
    "for i in range(0,500):\n",
    "    print(str(top_500[i]) + str(\" \") + str(top_500_count[i]))\n",
    "    \n",
    "print(\"Top 5 words representing each cluster and their feature values:\\n\")\n",
    "for i in range(0,10):\n",
    "    print(\"\\nCluster \"+str(i+1) + \":\\n\")\n",
    "    print(top_5[i]) \n",
    "    print(top_5_feature_values[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
