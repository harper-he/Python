{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IST 664 Natural Language Processing\n",
    "# Homework 4\n",
    "# Harper He\n",
    "# xhe128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import nltk\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# open the file and check the file type\n",
    "baby=open('baby.txt').readlines()\n",
    "print(type(baby))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewerID:A1HK2FQW6KXQB2\n",
      "asin:097293751X\n",
      "reviewerName:Amanda Johnsen \"Amanda E. Johnsen\"\n",
      "helpful:[0, 0]\n",
      "reviewText:Perfect for new parents. We were able to keep track of baby's feeding, sleep and dia\n"
     ]
    }
   ],
   "source": [
    "# convert the list back to string\n",
    "baby_str='' \n",
    "for i in baby:\n",
    "    baby_str=baby_str+i\n",
    "print(baby_str[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reviewerID:A1HK2FQW6KXQB2\\n',\n",
       " 'asin:097293751X\\n',\n",
       " 'reviewerName:Amanda Johnsen \"Amanda E. Johnsen\"\\n',\n",
       " 'helpful:[0, 0]\\n',\n",
       " \"reviewText:Perfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!\\n\",\n",
       " 'overall:5.0\\n',\n",
       " 'summary:Awesine\\n',\n",
       " 'unixReviewTime:1373932800\\n',\n",
       " 'reviewTime:07 16, 2013\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first 10 lines\n",
    "baby[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Perfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!\",\n",
       " 'This book is such a life saver.  It has been so helpful to be able to go back to track trends, answer pediatrician questions, or communicate with each other when you are up at different times of the night with a newborn.  I think it is one of those things that everyone should be required to have before they leave the hospital.  We went through all the pages of the newborn version, then moved to the infant version, and will finish up the second infant book (third total) right as our baby turns 1.  See other things that are must haves for baby at [...]',\n",
       " \"Helps me know exactly how my babies day has gone with my mother in law watching him while I go to work.  It also has a section for her to write notes and let me know anything she may need.  I couldn't be happier with this book.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracts only review texts\n",
    "reviews=[]\n",
    "with open(\"baby.txt\") as baby:\n",
    "    for line in baby:       \n",
    "        if(line.startswith(\"reviewText:\")):\n",
    "            each_line = line.split(\"reviewText:\")[1]\n",
    "            each_line = each_line.replace('\\n','')\n",
    "            reviews.append(each_line)\n",
    "print(len(reviews))\n",
    "reviews[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the list\n",
    "review_lower = [w.lower() for w in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"perfect for new parents. we were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. made life easier when the doctor would ask ques\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list to string for tokenization\n",
    "review_str='' \n",
    "for i in review_lower:\n",
    "    review_str=review_str+i\n",
    "review_str[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743494\n",
      "['perfect for new parents.']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent_tokens=sent_tokenize(review_str)\n",
    "print(len(sent_tokens))\n",
    "print(sent_tokens[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['perfect', 'for', 'new', 'parents', '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word level tokenization for each sentence\n",
    "sent_word_token = [word_tokenize(word) for word in sent_tokens]\n",
    "sent_word_token[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.0', '5.0', '5.0', '5.0', '4.0', '4.0', '5.0', '5.0', '3.0', '5.0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag the review as 'pos' or 'neg' based on the user's rating of the product\n",
    "# Extract overall part from the file\n",
    "baby_overall = re.findall(r'overall:(.*)',baby_str)\n",
    "baby_overall[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160796\n",
      "[(['perfect', 'for', 'new', 'parents', '.'], '5.0')]\n"
     ]
    }
   ],
   "source": [
    "# combine the sentence token and the rating\n",
    "baby_comb = list(zip(sent_word_token, baby_overall))\n",
    "print(len(baby_comb))\n",
    "print(baby_comb[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160792\n"
     ]
    }
   ],
   "source": [
    "# create a tag list based on the rating\n",
    "senti_tag=[]\n",
    "for a,b in baby_comb:\n",
    "    if b == '5.0' or b == '4.0':\n",
    "        senti_tag==senti_tag.append('pos')\n",
    "    elif b == '1.0' or b == '2.0':\n",
    "        senti_tag==senti_tag.append('neg')\n",
    "    elif b == '3.0':\n",
    "        senti_tag==senti_tag.append('neu')\n",
    "print(len(senti_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['perfect', 'for', 'new', 'parents', '.'], 'pos')]\n"
     ]
    }
   ],
   "source": [
    "# combine the sentence token and the tag\n",
    "review_senti=list(zip(sent_word_token, senti_tag))\n",
    "print(review_senti[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143537\n",
      "[(['perfect', 'for', 'new', 'parents', '.'], 'pos')]\n"
     ]
    }
   ],
   "source": [
    "# remove those neutral tokens\n",
    "rev_senti=[(token,tag) for (token,tag) in review_senti if tag=='pos' or tag=='neg']\n",
    "print(len(rev_senti))\n",
    "print(rev_senti[:1])\n",
    "# this is my final token to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_senti=random.sample(rev_senti,30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126525\n",
      "17012\n"
     ]
    }
   ],
   "source": [
    "# provide two lists of sentences, pos and neg\n",
    "pos_rev = [(token,tag) for (token,tag) in review_senti if tag=='pos']\n",
    "neg_rev = [(token,tag) for (token,tag) in review_senti if tag=='neg']\n",
    "print(len(pos_rev))\n",
    "print(len(neg_rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "### Classification 1\n",
    "### \"Bag of words\" features in the sentence_polarity corpus\n",
    "# get the sentence corpus and look at some sentences\n",
    "sentences = sentence_polarity.sents()\n",
    "print(len(sentences))\n",
    "print(type(sentences))\n",
    "print(sentence_polarity.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n"
     ]
    }
   ],
   "source": [
    "# look at the sentences by category to see how many positive and negative\n",
    "pos_sents = sentence_polarity.sents(categories='pos')\n",
    "print(len(pos_sents))\n",
    "neg_sents = sentence_polarity.sents(categories='neg')\n",
    "print(len(neg_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['simplistic', ',', 'silly', 'and', 'tedious', '.'], 'neg')\n",
      "(['provides', 'a', 'porthole', 'into', 'that', 'noble', ',', 'trembling', 'incoherence', 'that', 'defines', 'us', 'all', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "# setup the movie reviews sentences for classification\n",
    "# create a list of documents, each document is one sentence as a list of words paired with category\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "             for sent in sentence_polarity.sents(categories=cat)]\n",
    "# look at the first and last documents - consists of all the words in the review\n",
    "# followed by the category\n",
    "print(documents[0])\n",
    "print(documents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly reorder documents\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'that', 'it', 'as', 'but', 'with', 'film', 'this', 'for', 'its', 'an', 'movie', \"it's\", 'be', 'on', 'you', 'not', 'by', 'about', 'one', 'more', 'like', 'has', 'are', 'at', 'from', 'than', '\"', 'all', '--', 'his', 'have', 'so', 'if', 'or', 'story', 'i', 'too', 'just', 'who', 'into', 'what']\n"
     ]
    }
   ],
   "source": [
    "# get all words from all movie_reviews and put into a frequency distribution\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "# get the 2000 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features (keywords) of a document by using just the words, or a BOW/unigram baseline\n",
    "# each feature is 'contains(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in rev_senti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_train_set,word_test_set = featuresets[3000:],featuresets[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8713333333333333\n"
     ]
    }
   ],
   "source": [
    "classifier=nltk.NaiveBayesClassifier.train(word_train_set)\n",
    "print(nltk.classify.accuracy(classifier,word_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              V_ordinary = True              neg : pos    =     12.5 : 1.0\n",
      "               V_project = True              neg : pos    =     12.5 : 1.0\n",
      "                V_lesson = True              neg : pos    =     12.5 : 1.0\n",
      "                V_beauty = True              neg : pos    =     12.5 : 1.0\n",
      "             V_substance = True              neg : pos    =      7.5 : 1.0\n",
      "               V_capable = True              neg : pos    =      7.5 : 1.0\n",
      "               V_channel = True              neg : pos    =      7.5 : 1.0\n",
      "             V_painfully = True              neg : pos    =      7.5 : 1.0\n",
      "         V_old-fashioned = True              neg : pos    =      7.5 : 1.0\n",
      "           V_experiences = True              neg : pos    =      7.5 : 1.0\n",
      "                  V_rent = True              neg : pos    =      7.5 : 1.0\n",
      "               V_carries = True              neg : pos    =      7.5 : 1.0\n",
      "                 V_drags = True              neg : pos    =      7.5 : 1.0\n",
      "              V_nonsense = True              neg : pos    =      7.5 : 1.0\n",
      "              V_southern = True              neg : pos    =      7.5 : 1.0\n",
      "               V_monster = True              neg : pos    =      7.5 : 1.0\n",
      "                 V_skill = True              neg : pos    =      7.5 : 1.0\n",
      "               V_theater = True              neg : pos    =      7.5 : 1.0\n",
      "                  V_dull = True              neg : pos    =      7.5 : 1.0\n",
      "               V_capture = True              neg : pos    =      7.5 : 1.0\n",
      "              V_audience = True              neg : pos    =      7.5 : 1.0\n",
      "                 V_stale = True              neg : pos    =      7.5 : 1.0\n",
      "                   V_die = True              neg : pos    =      7.5 : 1.0\n",
      "              V_presents = True              neg : pos    =      7.5 : 1.0\n",
      "                V_merely = True              neg : pos    =      7.5 : 1.0\n",
      "               V_tribute = True              neg : pos    =      7.5 : 1.0\n",
      "           V_performance = True              neg : pos    =      7.5 : 1.0\n",
      "                V_battle = True              neg : pos    =      7.5 : 1.0\n",
      "                  V_lame = True              neg : pos    =      7.5 : 1.0\n",
      "              V_ensemble = True              neg : pos    =      7.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# show which features of classifier are most informative\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classification 2 - add features\n",
    "### Using a sentiment lexicon with scores or counts: Subjectivity\n",
    "SLpath=\"subjclueslen1-HLTEMNLP05.tff\"\n",
    "def readSubjectivity(path):\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL = readSubjectivity(SLpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6885"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words are in the dictionary\n",
    "len(SL.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in rev_senti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# show just the two sentiment lexicon features in document 0\n",
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "print(SL_featuresets[0][0]['negativecount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696666666666667"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrain the classifier using these features\n",
    "train_set, test_set = SL_featuresets[3000:], SL_featuresets[:3000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'a', 'difference', 'between', 'movies', 'with', 'the', 'courage', 'to', 'go', 'over', 'the', 'top', 'and', 'movies', 'that', \"don't\", 'care', 'about', 'being', 'stupid']\n",
      "['a', 'farce', 'of', 'a', 'parody', 'of', 'a', 'comedy', 'of', 'a', 'premise', ',', 'it', \"isn't\", 'a', 'comparison', 'to', 'reality', 'so', 'much', 'as', 'it', 'is', 'a', 'commentary', 'about', 'our', 'knowledge', 'of', 'films', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['most', 'of', 'the', 'problems', 'with', 'the', 'film', \"don't\", 'derive', 'from', 'the', 'screenplay', ',', 'but', 'rather', 'the', 'mediocre', 'performances', 'by', 'most', 'of', 'the', 'actors', 'involved']\n",
      "['the', 'lack', 'of', 'naturalness', 'makes', 'everything', 'seem', 'self-consciously', 'poetic', 'and', 'forced', '.', '.', '.', \"it's\", 'a', 'pity', 'that', \"[nelson's]\", 'achievement', \"doesn't\", 'match', 'his', 'ambition', '.']\n"
     ]
    }
   ],
   "source": [
    "###  Classification 3: Negation words - add features\n",
    "# Negation words \"not\", \"never\" and \"no\"\n",
    "# Not can appear in contractions of the form \"doesn't\"\n",
    "for sent in list(sentences)[:50]:\n",
    "   for word in sent:\n",
    "     if (word.endswith(\"n't\")):\n",
    "       print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this list of negation words includes some \"approximate negators\" like hardly and rarely\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One strategy with negation words is to negate the word following the negation word\n",
    "#   other strategies negate all words up to the next punctuation\n",
    "# Strategy is to go through the document words in order adding the word features,\n",
    "#   but if the word follows a negation words, change the feature to negated word\n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# define the feature sets\n",
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in rev_senti]\n",
    "# show the values of a couple of example features\n",
    "print(NOT_featuresets[0][0]['V_NOTcare'])\n",
    "print(NOT_featuresets[0][0]['V_always'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7816666666666666"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = NOT_featuresets[3000:], NOT_featuresets[:3000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             V_decathlon = False             neg : pos    =     17.4 : 1.0\n",
      "            V_inevitably = False             neg : pos    =     17.4 : 1.0\n",
      "          V_unattractive = False             neg : pos    =     17.4 : 1.0\n",
      "                 V_20lbs = False             neg : pos    =     17.4 : 1.0\n",
      "               V_NOTseen = True              neg : pos    =     13.9 : 1.0\n",
      "               V_NOTdoes = True              neg : pos    =     13.5 : 1.0\n",
      "                  V_ouml = False             neg : pos    =     13.5 : 1.0\n",
      "             V_meanwhile = False             neg : pos    =     12.5 : 1.0\n",
      "                V_speaks = False             neg : pos    =     12.5 : 1.0\n",
      "                 V_merry = False             neg : pos    =     12.5 : 1.0\n",
      "                V_lesson = True              neg : pos    =     12.5 : 1.0\n",
      "                V_select = False             neg : pos    =     12.5 : 1.0\n",
      "             V_estimated = False             neg : pos    =     12.5 : 1.0\n",
      "                V_tended = False             neg : pos    =     12.5 : 1.0\n",
      "             V_oversized = False             neg : pos    =     12.5 : 1.0\n",
      "                   V_cue = False             neg : pos    =     12.5 : 1.0\n",
      "                   V_hoo = False             neg : pos    =     12.5 : 1.0\n",
      "                V_NOTdue = True              neg : pos    =     12.5 : 1.0\n",
      "                V_doubts = False             neg : pos    =     12.5 : 1.0\n",
      "               V_hampers = False             neg : pos    =     12.5 : 1.0\n",
      "              V_marketed = False             neg : pos    =     12.5 : 1.0\n",
      "          V_nursery.this = False             neg : pos    =     12.5 : 1.0\n",
      "              V_playmats = False             neg : pos    =     12.5 : 1.0\n",
      "                   V_dvd = False             neg : pos    =     12.5 : 1.0\n",
      "               V_project = True              neg : pos    =     12.5 : 1.0\n",
      "          V_misalignment = False             neg : pos    =     12.5 : 1.0\n",
      "               V_slumber = False             neg : pos    =     12.5 : 1.0\n",
      "                  V_sunk = False             neg : pos    =     12.5 : 1.0\n",
      "                V_policy = False             neg : pos    =     12.5 : 1.0\n",
      "            V_assortment = False             neg : pos    =     12.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "###  Classification 4: Stop words removal\n",
    "# import the stop words list\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some negation words \n",
    "negationwords.extend(['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', \n",
    "                      'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
    "newstopwords = [word for word in stopwords if word not in negationwords]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words from the all words list\n",
    "new_all_words_list = [word for (sent,cat) in documents for word in sent if word not in newstopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue to define a new all words dictionary, get the 2000 most common as new_word_features\n",
    "new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "new_word_items = new_all_words.most_common(2000)\n",
    "new_word_features = [word for (word,count) in new_word_items] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rerun the \"bag of words\" \n",
    "def document_new_features(document, new_word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in new_word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features sets for a document, including keyword features and category feature\n",
    "new_featuresets = [(document_new_features(d, new_word_features), c) for (d, c) in rev_senti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_train_set,new_word_test_set = new_featuresets[3000:],new_featuresets[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.872\n"
     ]
    }
   ],
   "source": [
    "new_classifier=nltk.NaiveBayesClassifier.train(new_word_train_set)\n",
    "print(nltk.classify.accuracy(new_classifier,new_word_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             V_decathlon = False             neg : pos    =     17.4 : 1.0\n",
      "            V_inevitably = False             neg : pos    =     17.4 : 1.0\n",
      "          V_unattractive = False             neg : pos    =     17.4 : 1.0\n",
      "                 V_20lbs = False             neg : pos    =     17.4 : 1.0\n",
      "               V_NOTseen = True              neg : pos    =     13.9 : 1.0\n",
      "               V_NOTdoes = True              neg : pos    =     13.5 : 1.0\n",
      "                  V_ouml = False             neg : pos    =     13.5 : 1.0\n",
      "             V_meanwhile = False             neg : pos    =     12.5 : 1.0\n",
      "                V_speaks = False             neg : pos    =     12.5 : 1.0\n",
      "                 V_merry = False             neg : pos    =     12.5 : 1.0\n",
      "                V_lesson = True              neg : pos    =     12.5 : 1.0\n",
      "                V_select = False             neg : pos    =     12.5 : 1.0\n",
      "             V_estimated = False             neg : pos    =     12.5 : 1.0\n",
      "                V_tended = False             neg : pos    =     12.5 : 1.0\n",
      "             V_oversized = False             neg : pos    =     12.5 : 1.0\n",
      "                   V_cue = False             neg : pos    =     12.5 : 1.0\n",
      "                   V_hoo = False             neg : pos    =     12.5 : 1.0\n",
      "                V_NOTdue = True              neg : pos    =     12.5 : 1.0\n",
      "                V_doubts = False             neg : pos    =     12.5 : 1.0\n",
      "               V_hampers = False             neg : pos    =     12.5 : 1.0\n",
      "              V_marketed = False             neg : pos    =     12.5 : 1.0\n",
      "          V_nursery.this = False             neg : pos    =     12.5 : 1.0\n",
      "              V_playmats = False             neg : pos    =     12.5 : 1.0\n",
      "                   V_dvd = False             neg : pos    =     12.5 : 1.0\n",
      "               V_project = True              neg : pos    =     12.5 : 1.0\n",
      "          V_misalignment = False             neg : pos    =     12.5 : 1.0\n",
      "               V_slumber = False             neg : pos    =     12.5 : 1.0\n",
      "                  V_sunk = False             neg : pos    =     12.5 : 1.0\n",
      "                V_policy = False             neg : pos    =     12.5 : 1.0\n",
      "            V_assortment = False             neg : pos    =     12.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# show which features of classifier are most informative\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rerun the subjectivity to compare the accuracy\n",
    "def new_SL_features(document, new_word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in new_word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_SL_featuresets = [(new_SL_features(d, new_word_features, SL), c) for (d, c) in rev_senti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8703333333333333"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrain the classifier using these features\n",
    "new_train_set, new_test_set = new_SL_featuresets[3000:], new_SL_featuresets[:3000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(new_train_set)\n",
    "nltk.classify.accuracy(classifier, new_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rerun the representing negation \n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "def new_NOT_features(document, new_word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in new_word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in new_word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in new_word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# define the feature sets\n",
    "new_NOT_featuresets = [(new_NOT_features(d, new_word_features, negationwords), c) for (d, c) in rev_senti]\n",
    "# show the values of a couple of example features\n",
    "print(new_NOT_featuresets[0][0]['V_NOTcare'])\n",
    "print(new_NOT_featuresets[0][0]['V_always'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7696666666666667"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_set, new_test_set = new_NOT_featuresets[3000:], new_NOT_featuresets[:3000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(new_train_set)\n",
    "nltk.classify.accuracy(classifier, new_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             V_decathlon = False             neg : pos    =     17.4 : 1.0\n",
      "            V_inevitably = False             neg : pos    =     17.4 : 1.0\n",
      "          V_unattractive = False             neg : pos    =     17.4 : 1.0\n",
      "                 V_20lbs = False             neg : pos    =     17.4 : 1.0\n",
      "               V_NOTseen = True              neg : pos    =     13.9 : 1.0\n",
      "                  V_ouml = False             neg : pos    =     13.5 : 1.0\n",
      "               V_NOTdoes = False             neg : pos    =     13.5 : 1.0\n",
      "             V_meanwhile = False             neg : pos    =     12.5 : 1.0\n",
      "                 V_merry = False             neg : pos    =     12.5 : 1.0\n",
      "                V_lesson = True              neg : pos    =     12.5 : 1.0\n",
      "                V_select = False             neg : pos    =     12.5 : 1.0\n",
      "             V_estimated = False             neg : pos    =     12.5 : 1.0\n",
      "                V_tended = False             neg : pos    =     12.5 : 1.0\n",
      "             V_oversized = False             neg : pos    =     12.5 : 1.0\n",
      "                   V_cue = False             neg : pos    =     12.5 : 1.0\n",
      "                   V_hoo = False             neg : pos    =     12.5 : 1.0\n",
      "                V_NOTdue = True              neg : pos    =     12.5 : 1.0\n",
      "                V_speaks = True              neg : pos    =     12.5 : 1.0\n",
      "                V_doubts = False             neg : pos    =     12.5 : 1.0\n",
      "               V_hampers = False             neg : pos    =     12.5 : 1.0\n",
      "              V_marketed = False             neg : pos    =     12.5 : 1.0\n",
      "          V_nursery.this = False             neg : pos    =     12.5 : 1.0\n",
      "              V_playmats = False             neg : pos    =     12.5 : 1.0\n",
      "                   V_dvd = False             neg : pos    =     12.5 : 1.0\n",
      "               V_project = True              neg : pos    =     12.5 : 1.0\n",
      "          V_misalignment = False             neg : pos    =     12.5 : 1.0\n",
      "               V_slumber = False             neg : pos    =     12.5 : 1.0\n",
      "                  V_sunk = False             neg : pos    =     12.5 : 1.0\n",
      "                V_policy = False             neg : pos    =     12.5 : 1.0\n",
      "            V_assortment = False             neg : pos    =     12.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
