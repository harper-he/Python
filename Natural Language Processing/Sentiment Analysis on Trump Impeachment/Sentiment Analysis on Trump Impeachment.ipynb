{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 1 Tokenize\n",
    "import nltk\n",
    "f=open('C:/Users/65730/TrumpImpeach_Reddit_Comments.txt',encoding = \"utf-8\")\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['House Passes Trump Impeachment Inquiry, Moving to Public Phase', '>Practically speaking, the resolution outlines the rights and procedures that will guide the process from here on out, including the public presentation of evidence and how Mr. Trump and his legal team will be able to eventually mount a defense.\\\\n>\\\\n>But its significance was more profound: After five weeks of private fact-finding, an almost completely unified Democratic caucus signaled that, despite Republican opposition, they now have enough confidence in the severity of the underlying facts about Mr. Trump’s dealings with Ukraine to start making their case for impeachment in public.\\\\n\\\\nPaywall workaround:\\\\n\\\\nhttp://archive.is/6oLGL', 'What led to Trump’s first meeting on June 20, 2017, with Ukraine’s then-President Petro Poroshenko?\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  \"'House\",\n",
       "  'Passes',\n",
       "  'Trump',\n",
       "  'Impeachment',\n",
       "  'Inquiry',\n",
       "  ',',\n",
       "  'Moving',\n",
       "  'to',\n",
       "  'Public',\n",
       "  'Phase',\n",
       "  \"'\",\n",
       "  ',',\n",
       "  \"'\",\n",
       "  '>',\n",
       "  'Practically',\n",
       "  'speaking',\n",
       "  ',',\n",
       "  'the',\n",
       "  'resolution',\n",
       "  'outlines',\n",
       "  'the',\n",
       "  'rights',\n",
       "  'and',\n",
       "  'procedures',\n",
       "  'that',\n",
       "  'will',\n",
       "  'guide',\n",
       "  'the',\n",
       "  'process',\n",
       "  'from',\n",
       "  'here',\n",
       "  'on',\n",
       "  'out',\n",
       "  ',',\n",
       "  'including',\n",
       "  'the',\n",
       "  'public',\n",
       "  'presentation',\n",
       "  'of',\n",
       "  'evidence',\n",
       "  'and',\n",
       "  'how',\n",
       "  'Mr.',\n",
       "  'Trump',\n",
       "  'and',\n",
       "  'his',\n",
       "  'legal',\n",
       "  'team',\n",
       "  'will',\n",
       "  'be',\n",
       "  'able',\n",
       "  'to',\n",
       "  'eventually',\n",
       "  'mount',\n",
       "  'a',\n",
       "  'defense.\\\\n',\n",
       "  '>',\n",
       "  '\\\\n',\n",
       "  '>',\n",
       "  'But',\n",
       "  'its',\n",
       "  'significance',\n",
       "  'was',\n",
       "  'more',\n",
       "  'profound',\n",
       "  ':',\n",
       "  'After',\n",
       "  'five',\n",
       "  'weeks',\n",
       "  'of',\n",
       "  'private',\n",
       "  'fact-finding',\n",
       "  ',',\n",
       "  'an',\n",
       "  'almost',\n",
       "  'completely',\n",
       "  'unified',\n",
       "  'Democratic',\n",
       "  'caucus',\n",
       "  'signaled',\n",
       "  'that',\n",
       "  ',',\n",
       "  'despite',\n",
       "  'Republican',\n",
       "  'opposition',\n",
       "  ',',\n",
       "  'they',\n",
       "  'now',\n",
       "  'have',\n",
       "  'enough',\n",
       "  'confidence',\n",
       "  'in',\n",
       "  'the',\n",
       "  'severity',\n",
       "  'of',\n",
       "  'the',\n",
       "  'underlying',\n",
       "  'facts',\n",
       "  'about',\n",
       "  'Mr.',\n",
       "  'Trump',\n",
       "  '’',\n",
       "  's',\n",
       "  'dealings',\n",
       "  'with',\n",
       "  'Ukraine',\n",
       "  'to',\n",
       "  'start',\n",
       "  'making',\n",
       "  'their',\n",
       "  'case',\n",
       "  'for',\n",
       "  'impeachment',\n",
       "  'in',\n",
       "  'public.\\\\n\\\\nPaywall',\n",
       "  'workaround',\n",
       "  ':',\n",
       "  '\\\\n\\\\nhttp',\n",
       "  ':',\n",
       "  '//archive.is/6oLGL',\n",
       "  \"'\",\n",
       "  ',',\n",
       "  \"'What\",\n",
       "  'led',\n",
       "  'to',\n",
       "  'Trump',\n",
       "  '’',\n",
       "  's',\n",
       "  'first',\n",
       "  'meeting',\n",
       "  'on',\n",
       "  'June',\n",
       "  '20',\n",
       "  ',',\n",
       "  '2017',\n",
       "  ',',\n",
       "  'with',\n",
       "  'Ukraine',\n",
       "  '’',\n",
       "  's',\n",
       "  'then-President',\n",
       "  'Petro',\n",
       "  'Poroshenko',\n",
       "  '?']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=[nltk.word_tokenize(word)for word in sentences]\n",
    "words[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', \"'\", 'to', '``', 'a', 'and', 'of', 'is', 'that', 'in', 'I', '’', ':', 'he', 'it', 'Trump', \"'s\", 'for', '?', 'they', 'his', 'be', 'this', ')', 'on', 'are', '(', 'with', 's', 'was', 'you', 'have', \"''\", 'not', '!', 'as', '[', ']', 'will', 'him', '2020', \"n't\", 'https', '\\\\n\\\\n', 'Election', 'all', 'just', 'but']\n"
     ]
    }
   ],
   "source": [
    "## Task 2 Frequency distribution\n",
    "## get all words from all comments and put into a frequency distribution\n",
    "#   note lowercase, but no stemming or stopwords\n",
    "word1=nltk.word_tokenize(raw)\n",
    "all_words = nltk.FreqDist(word1)\n",
    "# get the 2000 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3 Sentiment classification \n",
    "#Sentiment Classification - Words as Features (BOW or unigram features)  \n",
    "#load the sentence_polarity corpus and create a list of documents   \n",
    "from nltk.corpus import sentence_polarity  \n",
    "import random  \n",
    "random.seed(30)   \n",
    "sentences=sentence_polarity.sents()  \n",
    "documents= [(sent,cat) for cat in sentence_polarity.categories()  \n",
    "         for sent in sentence_polarity.sents(categories=cat)]  \n",
    "random.shuffle(documents) #random shuffle documents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define the set of words that will be used for features   \n",
    "##limit to the 2000 most frequent words   \n",
    "##lowercase words without stemming or removing stopwords   \n",
    "all_words_list=[word for (sent,cat) in documents for word in sent]  \n",
    "all_words = nltk.FreqDist(all_words_list)  \n",
    "word_items = all_words.most_common(2000)  \n",
    "word_features = [word for (word,freq) in word_items]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727\n"
     ]
    }
   ],
   "source": [
    "#define the unigram features for each document, using just the words  \n",
    "#The feature label will be ‘contains(keyword)’ for each keyword (aka word) in the word_features set，the value is whether the word is contained in the document  \n",
    "def document_features(document,word_features):  \n",
    "    document_words=set(document)  \n",
    "    features={}  \n",
    "    for word in word_features:  \n",
    "        features['contain({})'.format(word)] = (word in document_words)  \n",
    "    return features  \n",
    "#Define the feature sets for the documents.  \n",
    "featuresets=[(document_features(d,word_features),c) for (d,c) in documents]  \n",
    "# do a 90/10 split of our approximately 10,000 documents  \n",
    "word_train_set,word_test_set = featuresets[1000:],featuresets[:1000]  \n",
    "classifier=nltk.NaiveBayesClassifier.train(word_train_set)  \n",
    "print(nltk.classify.accuracy(classifier,word_test_set))#0.727  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Classification - Subjectivity Count features  \n",
    "#read in the subjectivity words from the subjectivity lexicon  \n",
    "SLpath = 'C:/Users/65730/subjclueslen1-HLTEMNLP05.tff'  \n",
    "\n",
    "def readSubjectivity(path):  \n",
    "    flexicon = open(path, 'r')  \n",
    "    # initialize an empty dictionary  \n",
    "    sldict = { }  \n",
    "    for line in flexicon:  \n",
    "        fields = line.split()   # default is to split on whitespace  \n",
    "        # split each field on the '=' and keep the second part as the value  \n",
    "        strength = fields[0].split(\"=\")[1]  \n",
    "        word = fields[2].split(\"=\")[1]  \n",
    "        posTag = fields[3].split(\"=\")[1]  \n",
    "        stemmed = fields[4].split(\"=\")[1]  \n",
    "        polarity = fields[5].split(\"=\")[1]  \n",
    "        if (stemmed == 'y'):  \n",
    "            isStemmed = True  \n",
    "        else:  \n",
    "            isStemmed = False  \n",
    "        # put a dictionary entry with the word as the keyword  \n",
    "        #     and a list of the other values  \n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]  \n",
    "    return sldict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that include word counts of subjectivity words\n",
    "# negative feature will have number of weakly negative words +\n",
    "#    2 * number of strongly negative words\n",
    "# positive feature has similar definition\n",
    "#    not counting neutral words\n",
    "SL = readSubjectivity(SLpath) \n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features\n",
    "\n",
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show just the two sentiment lexicon features in document 0\n",
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "print(SL_featuresets[0][0]['negativecount'])\n",
    "SL_featuresets[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.762"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrain the classifier using these features\n",
    "SL_train_set, SL_test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "SL_classifier = nltk.NaiveBayesClassifier.train(SL_train_set)\n",
    "nltk.classify.accuracy(SL_classifier, SL_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'a', 'difference', 'between', 'movies', 'with', 'the', 'courage', 'to', 'go', 'over', 'the', 'top', 'and', 'movies', 'that', \"don't\", 'care', 'about', 'being', 'stupid']\n",
      "['a', 'farce', 'of', 'a', 'parody', 'of', 'a', 'comedy', 'of', 'a', 'premise', ',', 'it', \"isn't\", 'a', 'comparison', 'to', 'reality', 'so', 'much', 'as', 'it', 'is', 'a', 'commentary', 'about', 'our', 'knowledge', 'of', 'films', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['most', 'of', 'the', 'problems', 'with', 'the', 'film', \"don't\", 'derive', 'from', 'the', 'screenplay', ',', 'but', 'rather', 'the', 'mediocre', 'performances', 'by', 'most', 'of', 'the', 'actors', 'involved']\n",
      "['the', 'lack', 'of', 'naturalness', 'makes', 'everything', 'seem', 'self-consciously', 'poetic', 'and', 'forced', '.', '.', '.', \"it's\", 'a', 'pity', 'that', \"[nelson's]\", 'achievement', \"doesn't\", 'match', 'his', 'ambition', '.']\n",
      "0.762\n"
     ]
    }
   ],
   "source": [
    "###task 4 Negation features \n",
    "\n",
    "# Negation words \"not\", \"never\" and \"no\"\n",
    "# Not can appear in contractions of the form \"doesn't\"\n",
    "for sent in list(sentences)[:50]:\n",
    "   for word in sent:\n",
    "     if (word.endswith(\"n't\")):\n",
    "       print(sent)\n",
    "\n",
    "# this list of negation words includes some \"approximate negators\" like hardly and rarely\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "\n",
    "# One strategy with negation words is to negate the word following the negation word\n",
    "#   other strategies negate all words up to the next punctuation\n",
    "# Strategy is to go through the document words in order adding the word features,\n",
    "#   but if the word follows a negation words, change the feature to negated word\n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features\n",
    "\n",
    "# define the feature sets\n",
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]\n",
    "\n",
    "not_train_set, not_test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "NOT_classifier  = nltk.NaiveBayesClassifier.train(not_train_set)\n",
    "print(nltk.classify.accuracy(NOT_classifier, not_test_set)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words and re-run SL_features  \n",
    "#remove stop words in word features using negation filter  \n",
    "stopwords = nltk.corpus.stopwords.words('english')  \n",
    "newstopwords = [word for word in stopwords if word not in negationwords]  \n",
    "new_all_words_list=[word for word in all_words_list if word not in newstopwords]  \n",
    "#get new word features of length 2000 after the stopwords are removed  \n",
    "new_all_words = nltk.FreqDist(new_all_words_list)  \n",
    "new_word_items = new_all_words.most_common(2000)  \n",
    "new_word_features = [word for (word,count) in new_word_items]  \n",
    "#re run SL_features  \n",
    "def newSL_features(document, new_word_features, SL):  \n",
    "    document_words = set(document)  \n",
    "    features = {}  \n",
    "    for word in new_word_features:  \n",
    "        features['contains({})'.format(word)] = (word in document_words)  \n",
    "    # count variables for the 4 classes of subjectivity  \n",
    "    weakPos = 0  \n",
    "    strongPos = 0  \n",
    "    weakNeg = 0  \n",
    "    strongNeg = 0  \n",
    "    for word in document_words:  \n",
    "        if word in SL:  \n",
    "            strength, posTag, isStemmed, polarity = SL[word]  \n",
    "            if strength == 'weaksubj' and polarity == 'positive':  \n",
    "                weakPos += 1  \n",
    "            if strength == 'strongsubj' and polarity == 'positive':  \n",
    "                strongPos += 1  \n",
    "            if strength == 'weaksubj' and polarity == 'negative':  \n",
    "                weakNeg += 1  \n",
    "            if strength == 'strongsubj' and polarity == 'negative':  \n",
    "                strongNeg += 1  \n",
    "            features['positivecount'] = weakPos + (2 * strongPos)  \n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)  \n",
    "    return features  \n",
    "newSL_featuresets = [(newSL_features(d, new_word_features, SL), c) for (d, c) in documents]  \n",
    "newSL_train_set, newSL_test_set = newSL_featuresets[1000:],newSL_featuresets[:1000]  \n",
    "newSLclassifier = nltk.NaiveBayesClassifier.train(newSL_train_set)  \n",
    "print(nltk.classify.accuracy(newSLclassifier, newSL_test_set)) #0.749  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<366>128 |\n",
      "pos | 147<359>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Subjectivity Count features- SL_classifier precision, recall and F-measure  \n",
    "# evaluation measures showing performance of negation  \n",
    "from nltk.metrics import *  \n",
    "reflist = []  \n",
    "testlist = []  \n",
    "for (features, label) in SL_test_set:  \n",
    "    reflist.append(label)  \n",
    "    testlist.append(NOT_classifier.classify(features))  \n",
    "# define and print confusion matrix  \n",
    "cm = ConfusionMatrix(reflist, testlist)  \n",
    "print(cm)  \n",
    "refpos = set()  \n",
    "refneg = set()  \n",
    "testpos = set()  \n",
    "testneg = set()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(reflist):  \n",
    "    if label == 'pos': refpos.add(i)  \n",
    "    if label == 'neg': refneg.add(i)  \n",
    "  \n",
    "for i, label in enumerate(testlist):  \n",
    "    if label == 'pos': testpos.add(i)  \n",
    "    if label == 'neg': testneg.add(i)  \n",
    "# compute precision, recall and F-measure for each label  \n",
    "def printmeasures(label, refset, testset):  \n",
    "    print(label, 'precision:', precision(refset, testset))  \n",
    "    print(label, 'recall:', recall(refset, testset))  \n",
    "    print(label, 'F-measure:', f_measure(refset, testset))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.7371663244353183\n",
      "pos recall: 0.7094861660079052\n",
      "pos F-measure: 0.7230614300100705\n"
     ]
    }
   ],
   "source": [
    "printmeasures('pos', refpos, testpos)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2875\n",
      "9202\n"
     ]
    }
   ],
   "source": [
    "#Predict reddit comments with SL_Classifier   \n",
    "#Get the predict results after applying classifier  \n",
    "pos_list=[]  \n",
    "neg_list=[]  \n",
    "for comments in words:  \n",
    "    if(SL_classifier.classify(SL_features(comments,word_features,SL))=='pos'):  \n",
    "        #use string.join() combine list  \n",
    "       pos_list.append(\" \".join(comments))  \n",
    "    if(SL_classifier.classify(SL_features(comments,word_features,SL))=='neg'):  \n",
    "       neg_list.append(\" \".join(comments))  \n",
    "  \n",
    "print(len(pos_list))  \n",
    "print(len(neg_list))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "df_pos=pd.DataFrame({'$Positive':pos_list})  \n",
    "df_neg=pd.DataFrame({'$Negative':neg_list})  \n",
    "df_pos.to_csv('CommentPostive.csv',index=False)  \n",
    "df_neg.to_csv('CommentNegative.csv',index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
