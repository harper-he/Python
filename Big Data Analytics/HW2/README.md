This is homework 2 of Big Data Analytics course.

In the "IST-718 Spring 2020 Homework 2 Map Reduce" assignment, we studied the log returns of 10 stocks during one year (365 days). I did:

1. transform an **RDD** that contains the contents of a text file and adjust the format.
2. define map functions
3. create three map-reduce jobs that will produce RDDs that contain the maximum, minimum and the total (sum) of a variable
4. Generate an RDD that calculated the cumulative sum

In the "data_cleaning" assignment, we read data from non-standard formats, cleaned data, and produced some basic analysis of it. I did:
1. use Spark 1.6 (sparkContext on variable sc) to load text files 
2. Filter out the lines that contain errors 
3. Define a function that created a key-value RDD 
4. Join two RDDs 
5. conduct filtered version of the joined RDDs
6. Use pyspark.ml.functions for computing the correlation between columns, computing absolute values.

In the "map_reduce_1" assignment, I did:
1. Construct a MapReduce job that takes an RDD and returns the weighted avereage.

Thank you!
